{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Billion Something-Something\n",
    "\n",
    "Script for processing the 20bn dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display video grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import typing\n",
    "\n",
    "from gpred import video_utils\n",
    "from env import twentybn\n",
    "\n",
    "def display_video_grid(\n",
    "    labels: twentybn.dataset.Labels,\n",
    "    action_instances: typing.List[typing.List[int]],\n",
    "    path: pathlib.Path,\n",
    "    num_rows: int = 5\n",
    "):\n",
    "    \"\"\"Displays 3 x N grid of videos.\n",
    "    \n",
    "    Args:\n",
    "        labels: 20BN labels.\n",
    "        action_instances: List of video ids per action.\n",
    "        path: Path of videos.\n",
    "        num_rows: Number of rows to display per batch.\n",
    "    \"\"\"\n",
    "    from IPython.display import clear_output\n",
    "    import ipywidgets as widgets\n",
    "    \n",
    "    next_button = widgets.Button(description=\"Next\")\n",
    "    \n",
    "    def assign_button_handler(id_action: int):\n",
    "        \"\"\"Assigns click handler to 'Next' button.\"\"\"\n",
    "        \n",
    "        SIZE_BATCH = 3 * num_rows\n",
    "        num_examples = len(action_instances[id_action])\n",
    "        idx_example_start = 0\n",
    "        \n",
    "        def show_next_video_callback(b: widgets.Button):\n",
    "            \"\"\"Called on button click to display next video grid.\"\"\"\n",
    "            nonlocal idx_example_start\n",
    "            with output:\n",
    "                clear_output()\n",
    "\n",
    "                print(f\"\\n{labels.actions[id_action].template}\")\n",
    "                print(f\"Examples {idx_example_start}..{idx_example_start + SIZE_BATCH - 1} out of {num_examples}\\n\")\n",
    "\n",
    "                idx_examples = range(idx_example_start, idx_example_start + SIZE_BATCH)\n",
    "                id_videos = [action_instances[id_action][idx_example] for idx_example in idx_examples]\n",
    "\n",
    "                video_utils.display_video_grid(id_videos, path, labels=[labels.videos[id_video].action_name for id_video in id_videos])\n",
    "\n",
    "                idx_example_start += SIZE_BATCH\n",
    "        \n",
    "        next_button._click_handlers.callbacks = []\n",
    "        next_button.on_click(show_next_video_callback)\n",
    "\n",
    "    input_action = widgets.BoundedIntText(value=0, min=0, max=len(labels.actions), description=\"Action index:\")\n",
    "    output = widgets.interactive_output(assign_button_handler, {\"id_action\": input_action})\n",
    "\n",
    "    return widgets.VBox([widgets.HBox([input_action, next_button]), output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20BN Something Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import config\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "\"\"\"\n",
    "sth_sth_labels = {\n",
    "    \"{id_action}\": \"Holding something next to something\"\n",
    "}\n",
    "\"\"\"\n",
    "with open(paths.data / \"labels/something-something-v2-labels.json\", \"r\") as f:\n",
    "    sth_sth_labels = json.load(f)\n",
    "\n",
    "\"\"\"\n",
    "sth_sth = [\n",
    "    {\n",
    "        \"id\": \"78687\",\n",
    "        \"label\": \"holding potato next to vicks vaporub bottle\",\n",
    "        \"template\": \"Holding [something] next to [something]\",\n",
    "        \"placeholders\": [\"potato\", \"vicks vaporub bottle\"],\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "with open(paths.data / \"labels/something-something-v2-train.json\", \"r\") as f:\n",
    "    sth_sth_train = json.load(f)\n",
    "\n",
    "with open(paths.data / \"labels/something-something-v2-validation.json\", \"r\") as f:\n",
    "    sth_sth_val = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\"\"\"\n",
    "sth_else = {\n",
    "    \"{id}\": [\n",
    "        {\n",
    "            \"name\": \"{id}/####.jpg\",\n",
    "            \"labels\": [\n",
    "                {\n",
    "                    \"box2d\": {\n",
    "                        \"x1\": float,\n",
    "                        \"x2\": float,\n",
    "                        \"y1\": float,\n",
    "                        \"y2\": float,\n",
    "                    },\n",
    "                    \"category\": \"battery\",\n",
    "                    \"gt_annotation\": \"object 0\",\n",
    "                    \"standard_category\": \"0000\",\n",
    "                }\n",
    "            ],\n",
    "            \"gt_placeholders\": [\"battery\"],\n",
    "            \"nr_instances\": 1},\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "sth_else = {}\n",
    "for i in tqdm.notebook.tqdm(range(4)):\n",
    "    with open(paths.data / f\"something_else/bounding_box_smthsmth_part{i+1}.json\", \"r\") as f:\n",
    "        for key, frames in json.load(f).items():\n",
    "            sth_else[key] = frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create template => idx_action map.\n",
    "idx_actions = {}\n",
    "for sth_sth_label in sth_sth_val:\n",
    "    fine_label = sth_sth_label[\"label\"]\n",
    "    template = sth_sth_label[\"template\"]\n",
    "    \n",
    "    coarse_label = re.sub(\"[\\[\\]]\", \"\", template)\n",
    "    idx_action = int(sth_sth_labels[coarse_label])\n",
    "    idx_actions[template] = idx_action\n",
    "\n",
    "# Create action labels.\n",
    "\"\"\"\n",
    "action_labels = [\n",
    "    {\n",
    "        \"label\": \"Approaching something with your camera\",\n",
    "        \"template\": \"Approaching [something] with your camera\",\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "action_labels = [None] * len(sth_sth_labels)\n",
    "for sth_sth_label in sth_sth_train:\n",
    "    if not None in action_labels:\n",
    "        break\n",
    "    \n",
    "    template = sth_sth_label[\"template\"]\n",
    "    coarse_label = re.sub(\"[\\[\\]]\", \"\", template)\n",
    "    idx_action = idx_actions[template]\n",
    "    \n",
    "    action_labels[idx_action] = {\n",
    "        \"label\": coarse_label,\n",
    "        \"template\": template\n",
    "    }\n",
    "\n",
    "# Create video labels.\n",
    "def process_labels(sth_sth_set, sth_else, idx_actions, video_labels):\n",
    "    labels = []\n",
    "    for sth_sth_label in tqdm.tqdm(sth_sth_set):\n",
    "        id_video = int(sth_sth_label[\"id\"])\n",
    "        if not str(id_video) in sth_else:\n",
    "            continue\n",
    "\n",
    "        template = sth_sth_label[\"template\"]\n",
    "        id_action = idx_actions[template]\n",
    "        placeholders = sth_sth_label[\"placeholders\"]\n",
    "\n",
    "        sth_else_label = sth_else[str(id_video)]\n",
    "        objects = sth_else_label[0][\"gt_placeholders\"]\n",
    "\n",
    "        frames = {}\n",
    "        for sth_else_frame in sth_else_label:\n",
    "            idx_frame = int(re.match(r\"\\d+/(\\d+)\\.jpg\", sth_else_frame[\"name\"])[1]) - 1\n",
    "            boxes = {}\n",
    "            for sth_else_box in sth_else_frame[\"labels\"]:\n",
    "                idx_obj = sth_else_box[\"standard_category\"]\n",
    "                if idx_obj != \"hand\":\n",
    "                    # Simplify integer. JSON key value still need to be strings.\n",
    "                    idx_obj = str(int(idx_obj))\n",
    "\n",
    "                box = sth_else_box[\"box2d\"]\n",
    "                boxes[idx_obj] = [[box[\"x1\"], box[\"y1\"]], [box[\"x2\"], box[\"y2\"]]]\n",
    "            \n",
    "            frames[idx_frame] = boxes\n",
    "\n",
    "        labels.append(id_video)\n",
    "        video_labels[id_video] = {\n",
    "            \"id_action\": id_action,\n",
    "            \"placeholders\": placeholders,\n",
    "            \"objects\": objects,\n",
    "            \"frames\": frames,\n",
    "        }\n",
    "    return labels\n",
    "\n",
    "\"\"\"\n",
    "video_labels = {\n",
    "    {id_video}: {\n",
    "        \"id_action\": id_action,\n",
    "        \"placeholders\": [\"a potato\", \"a vicks vaporub bottle\"],\n",
    "        \"objects\": [\"potato\", \"bottle\"],\n",
    "        \"frames\": {\n",
    "            idx_frame: {\n",
    "                \"{id_object/hand}\": [[x1, y1], [x2, y2]],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "train_set = [{video_id}, ...]\n",
    "val_set = [{video_id}, ...]\n",
    "\"\"\"\n",
    "unsorted_video_labels = {}\n",
    "train_set = process_labels(sth_sth_train, sth_else, idx_actions, unsorted_video_labels)\n",
    "val_set = process_labels(sth_sth_val, sth_else, idx_actions, unsorted_video_labels)\n",
    "\n",
    "video_labels = {}\n",
    "for key in sorted(unsorted_video_labels.keys()):\n",
    "    video_labels[key] = unsorted_video_labels[key]\n",
    "\n",
    "# Create action instances map.\n",
    "\"\"\"\n",
    "action_instances = [\n",
    "    [{id_video}, ...]\n",
    "]\n",
    "\"\"\"\n",
    "action_instances = [[] for _ in range(len(action_labels))]\n",
    "for id_video, video_label in video_labels.items():\n",
    "    id_action = video_label[\"id_action\"]\n",
    "    action_instances[id_action].append(id_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open(paths.data / \"action_labels.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(action_labels, f)\n",
    "# with open(paths.data / \"action_instances.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(action_instances, f)\n",
    "# with open(paths.data / \"video_labels.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(video_labels, f)\n",
    "\n",
    "with open(paths.data / \"train_set.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_set, f)\n",
    "with open(paths.data / \"val_set.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from env import twentybn\n",
    "from gpred import video_utils\n",
    "\n",
    "def create_video_labels_dataset(video_labels: typing.Dict, action_labels: typing.List, action_instances: typing.List):\n",
    "    \"\"\"Stores the Something-Else labels in h5py format.\n",
    "\n",
    "    h5py = {\n",
    "        \"actions\": {\n",
    "            \"id_action\": {\n",
    "                attrs: {\n",
    "                    \"id_action\": int,\n",
    "                    \"template\": utf8,\n",
    "                },\n",
    "                \"videos\": [V] (num_videos) uint32,\n",
    "            }\n",
    "        },\n",
    "        \"videos\": {\n",
    "            \"id_video\": {\n",
    "                attrs: {\n",
    "                    \"id_video\": int,\n",
    "                    \"id_action\": int,\n",
    "                },\n",
    "                \"objects\": [O] (num_objects) utf8,\n",
    "                \"keyframes\": [T] (num_keyframes) uint32,\n",
    "                \"boxes\": [T, 1 + O, 4] (num_keyframes, hand/num_objects, x1/y1/x2/y2) float32,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    Args:\n",
    "        video_labels: Something-Else labels.\n",
    "    \"\"\"\n",
    "    with h5py.File(paths.data / \"labels.hdf5\", \"w\") as f:\n",
    "        # Prepare action labels.\n",
    "        dset_actions = f.create_group(\"actions\")\n",
    "        A = len(action_labels)\n",
    "        for id_action in tqdm.notebook.tqdm(range(A)):\n",
    "            grp = dset_actions.create_group(str(id_action))\n",
    "            grp.attrs.create(\"id_action\", id_action, dtype=np.uint32)\n",
    "            grp.attrs[\"template\"] = action_labels[id_action][\"template\"]\n",
    "            grp.create_dataset(\"videos\", data=np.array(action_instances[id_action], dtype=np.uint32))\n",
    "        \n",
    "        # Prepare video labels.\n",
    "        dset_videos = f.create_group(\"videos\")\n",
    "        for id_video in tqdm.notebook.tqdm(video_labels):\n",
    "            label = video_labels[id_video]\n",
    "            grp = dset_videos.create_group(str(id_video))\n",
    "            grp.attrs.create(\"id_video\", id_video, dtype=np.uint32)\n",
    "            grp.attrs.create(\"id_action\", label[\"id_action\"], dtype=np.uint32)\n",
    "            \n",
    "            O = len(label[\"objects\"])\n",
    "            grp.attrs.create(\"objects\", label[\"objects\"], shape=(O,), dtype=h5py.string_dtype(encoding=\"utf-8\"))\n",
    "            \n",
    "            # Get keyframes from actual video.\n",
    "            keyframes_video = video_utils.get_keyframes(paths.data / \"videos\" / f\"{id_video}.webm\")\n",
    "            keyframes = []\n",
    "            boxes = []\n",
    "            for keyframe in label[\"frames\"]:\n",
    "                if not keyframe in keyframes_video:\n",
    "                    continue\n",
    "                keyframes.append(keyframe)\n",
    "                boxes_t = np.full((1 + O, 4), -float(\"inf\"), dtype=np.float32)\n",
    "                for obj, bbox in label[\"frames\"][keyframe].items():\n",
    "                    idx_obj = twentybn.utils.object_id_to_idx(obj)\n",
    "                    boxes_t[idx_obj] = np.array(bbox, dtype=np.float32).flatten()\n",
    "                boxes.append(boxes_t)\n",
    "            \n",
    "            grp.create_dataset(\"keyframes\", data=keyframes, dtype=np.uint32)\n",
    "            grp.create_dataset(\"boxes\", data=boxes, shape=(len(boxes), 1 + O, 4), dtype=np.float32)\n",
    "        \n",
    "        f.create_dataset(\"video_ids\", data=list(video_labels.keys()), dtype=np.uint32)\n",
    "\n",
    "create_video_labels_dataset(video_labels, action_labels, action_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract pre and post frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import symbolic\n",
    "\n",
    "from apps.twentybn import hand_detector\n",
    "from env import twentybn\n",
    "from gpred import video_utils, dnf_utils\n",
    "import config\n",
    "\n",
    "def point_inside_rectangle(box: np.ndarray, points: np.ndarray) -> typing.Union[bool, np.ndarray]:\n",
    "    \"\"\"Checks whether the points fall inside the rectangle.\n",
    "\n",
    "    Args:\n",
    "        box: [4] (x1/y1/x2/y2) corners.\n",
    "        points: [N, 2] or [2] (x/y).\n",
    "    Returns:\n",
    "        Boolean if one point is given, array of booleans [N] otherwise.\n",
    "    \"\"\"\n",
    "    # One point.\n",
    "    if points.shape == (2,):\n",
    "        return box[0] <= points[0] and points[0] <= box[2] and box[1] <= points[1] and points[1] <= box[3]\n",
    "    return (box[0] <= points[:,0]) & (points[:,0] <= box[2]) & (box[1] <= points[:,1]) & (points[:,1] <= box[3])\n",
    "\n",
    "def box_circle_collision(box: np.ndarray, circle: typing.Tuple[np.ndarray, float]) -> bool:\n",
    "    \"\"\"Checks whether the box and circle collide.\n",
    "    \n",
    "    Args:\n",
    "        box: [4] (x1/y1/x2/y2) corners.\n",
    "        circle: ([2] (x/y) center, radius).\n",
    "    Returns:\n",
    "        True if the shapes collide.\n",
    "    \"\"\"    \n",
    "    def line_segment_circle_collision(line_segment: np.ndarray, circle: typing.Tuple[np.ndarray, float]) -> bool:\n",
    "        \"\"\"Checks whether the line segment and circle collide.\n",
    "        \n",
    "        Args:\n",
    "            line_segment: [4] (x1/y1/x2/y2) endpoints.\n",
    "            circle: ([2] (x/y) center, radius).\n",
    "        Returns:\n",
    "            True if the shapes collide.\n",
    "        \"\"\"\n",
    "        # [[x1, y1], [x2, y2]]\n",
    "        endpoints = np.reshape(line_segment, (2, 2))\n",
    "        #print(\"endpoints:\", endpoints)\n",
    "        \n",
    "        # [cx, cy]\n",
    "        center, radius = circle\n",
    "        #print(\"center:\", center)\n",
    "        r2 = radius * radius\n",
    "        #print(\"radius2:\", r2)\n",
    "        \n",
    "        # [[x1 - cx, y1 - cy], [x2 - cx, y2 - cy]]\n",
    "        dc = center[None, :] - endpoints\n",
    "        dd = np.sum(dc * dc, axis=1)\n",
    "        idx_min = np.argmin(dd)\n",
    "        #print(\"min:\", idx_min, dd)\n",
    "        \n",
    "        # Check if closer endpoint is within radius.\n",
    "        if dd[idx_min] < r2:\n",
    "            return True\n",
    "        \n",
    "        # [x, y]\n",
    "        origin = endpoints[idx_min]\n",
    "        v_line = endpoints[1 - idx_min] - origin\n",
    "        v_line /= np.linalg.norm(v_line)\n",
    "        v_circle = center - origin\n",
    "        \n",
    "        # Check if projection of circle onto line falls outside the segment.\n",
    "        d_circle_line = v_line.dot(v_circle)\n",
    "        if d_circle_line < 0:\n",
    "            #print(\":\", d_circle_line)\n",
    "            return False\n",
    "        \n",
    "        # Orthogonal distance between circle and line.\n",
    "        d_circle = v_circle - d_circle_line * v_line\n",
    "        #print(\"::\", d_circle.dot(d_circle), r2)\n",
    "        return d_circle.dot(d_circle) < r2\n",
    "    \n",
    "    x1, y1, x2, y2 = box\n",
    "    return (\n",
    "        point_inside_rectangle(box, circle[0]) or\n",
    "        line_segment_circle_collision(np.array([x1, y1, x1, y2]), circle) or\n",
    "        line_segment_circle_collision(np.array([x2, y1, x2, y2]), circle) or\n",
    "        line_segment_circle_collision(np.array([x1, y1, x2, y1]), circle) or\n",
    "        line_segment_circle_collision(np.array([x1, y2, x2, y2]), circle)\n",
    "    )\n",
    "\n",
    "def box_box_collision(box_a: np.ndarray, box_b: np.ndarray) -> bool:\n",
    "    \"\"\"Checks whether the two boxes collide.\n",
    "    \n",
    "    Args:\n",
    "        box_a: [4] (x1/y1/x2/y2) corners.\n",
    "        box_b: [4] (x1/y1/x2/y2) corners.\n",
    "    Return:\n",
    "        Whether the two boxes collide.\n",
    "    \"\"\"\n",
    "    minkowski_0 = box_a[:2] - box_b[2:]\n",
    "    minkowski_1 = box_a[2:] - box_b[:2]\n",
    "    return (np.sign(minkowski_0) != np.sign(minkowski_1)).all()\n",
    "\n",
    "def box_hand_collision(box: np.ndarray, hand: hand_detector.Hand, radius: float = 15) -> bool:\n",
    "    \"\"\"Checks whether the box overlaps with any of the fingertips.\n",
    "    \n",
    "    Args:\n",
    "        box: [4] (x1/y1/x2/y2) corners.\n",
    "        hand: Detected hand.\n",
    "        radius: Distance from fingertips.\n",
    "    Returns:\n",
    "        Whether the box overlaps with any of the fingertips.\n",
    "    \"\"\"\n",
    "    for fingertip in hand.fingertips():\n",
    "        if box_circle_collision(box, (fingertip, radius)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def identify_contained_hand(box: np.ndarray, detected_hands: typing.List[hand_detector.Hand]) -> typing.Optional[hand_detector.Hand]:\n",
    "    \"\"\"Identifies which hand is contained inside the bounding box.\n",
    "    \n",
    "    Args:\n",
    "        box: [4] (x1/y1/x2/y2) corners.\n",
    "        detected_hands: Detected hands output by `hand_detector.HandDetector`.\n",
    "    Returns:\n",
    "        Hand corresponding to the one in the bounding box if any.\n",
    "    \"\"\"\n",
    "    is_contained = np.zeros((len(detected_hands),), dtype=int)\n",
    "    for i, hand in enumerate(detected_hands):\n",
    "        points = np.concatenate((hand.palm(), hand.fingertips()), axis=0)\n",
    "        is_contained[i] = point_inside_rectangle(box, points).sum()\n",
    "    \n",
    "    if is_contained.sum() == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_max = is_contained.argmax()\n",
    "    return detected_hands[idx_max]\n",
    "\n",
    "class PropositionTestFailure(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "class PropositionUnknown(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "def is_sth_visible(boxes: np.ndarray, idx_object: int, expected: typing.Optional[bool] = None) -> bool:\n",
    "    \"\"\"Checks whether the specified object is visible.\n",
    "    \n",
    "    Raises a PropositionTestFailure if the expected result is specified and does not match the test result.\n",
    "    \n",
    "    Args:\n",
    "        boxes: [4, 4] (hand/a/b/c, x1/y1/x2/y2) box corners.\n",
    "        idx_object: Object index (0/1/2/3 for \"hand\"/\"a\"/\"b\"/\"c\").\n",
    "        expected: Expected result.\n",
    "    \"\"\"\n",
    "    result = idx_object < boxes.shape[0] and boxes[idx_object, 0] >= 0\n",
    "    \n",
    "    if expected is not None and result != expected:\n",
    "        raise PropositionTestFailure(f\"visible({idx_object}) != {expected}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def is_sth_touching_hand(\n",
    "    boxes: np.ndarray,\n",
    "    idx_object: int,\n",
    "    hand: typing.Optional[hand_detector.Hand],\n",
    "    expected: typing.Optional[bool] = None\n",
    ") -> bool:\n",
    "    \"\"\"Checks whether the specified object is touching the hand.\n",
    "    \n",
    "    Raises a PropositionTestFailure if the expected result is specified and does not match the test result.\n",
    "    \n",
    "    Args:\n",
    "        boxes: [4, 4] (hand/a/b/c, x1/y1/x2/y2) box corners.\n",
    "        idx_object: Object index (0/1/2/3 for \"hand\"/\"a\"/\"b\"/\"c\").\n",
    "        hand: Detected hand, if it exists.\n",
    "        expected: Expected result.\n",
    "    \"\"\"\n",
    "    if idx_object >= boxes.shape[0] or boxes[0, 0] < 0 or boxes[idx_object, 0] < 0:\n",
    "        result = False\n",
    "    elif hand is None:\n",
    "        raise PropositionUnknown(f\"touching({'abc'[idx_object-1]}, hand): No hand detected.\")\n",
    "    else:\n",
    "        box_hand = boxes[0, :]\n",
    "        box_obj = boxes[idx_object, :]\n",
    "\n",
    "        # If boxes don't collide, then they are not touching.\n",
    "        result = box_box_collision(box_hand, box_obj) and box_hand_collision(box_obj, hand)\n",
    "    \n",
    "    if expected is not None and result != expected:\n",
    "        raise PropositionTestFailure(f\"touching({'abc'[idx_object-1]}, hand) != {expected}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def is_sth_touching_sth(boxes: np.ndarray, idx_object_a: int, idx_object_b, expected: typing.Optional[bool] = None) -> bool:\n",
    "    \"\"\"Checks whether one object is touching another.\n",
    "    \n",
    "    Returns false if the objects are not overlapping, otherwise raises a PropositionUnknown.\n",
    "    Raises a PropositionTestFailure if the expected result is specified and does not match the test result.\n",
    "    \n",
    "    Args:\n",
    "        boxes: [4, 4] (hand/a/b/c, x1/y1/x2/y2) box corners.\n",
    "        idx_object_a: Object index (0/1/2/3 for \"hand\"/\"a\"/\"b\"/\"c\").\n",
    "        idx_object_b: Object index (0/1/2/3 for \"hand\"/\"a\"/\"b\"/\"c\").\n",
    "        expected: Expected result.\n",
    "    \"\"\"\n",
    "    if max(idx_object_a, idx_object_b) >= boxes.shape[0] or boxes[idx_object_a, 0] < 0 or boxes[idx_object_b, 0] < 0:\n",
    "        raise PropositionUnknown(f\"touching({'abc'[idx_object_a-1]}, {'abc'[idx_object_b-1]}): Missing object.\")\n",
    "    \n",
    "    box_a = boxes[idx_object_a, :]\n",
    "    box_b = boxes[idx_object_b, :]\n",
    "    # If boxes don't collide, then they are not touching.\n",
    "    result = box_box_collision(box_a, box_b)\n",
    "    if result:\n",
    "        raise PropositionUnknown(f\"touching({'abc'[idx_object_a-1]}, {'abc'[idx_object_b-1]}): Unable to determine from overlapping boxes.\")\n",
    "    \n",
    "    if expected is not None and result != expected:\n",
    "        raise PropositionTestFailure(f\"touching({'abc'[idx_object_a-1]}, {'abc'[idx_object_b-1]}) != {expected}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def generate_tests(\n",
    "    pddl: symbolic.Pddl,\n",
    "    s_partial: typing.Optional[np.ndarray] = None\n",
    ") -> typing.List[typing.Tuple[int, typing.Callable[[typing.Dict, bool], bool]]]:\n",
    "    \"\"\"Generate tests for detecting start/end frames.\n",
    "    \n",
    "    The first test evaluates whether 'a' is touching the hand by testing whether their bounding boxes overlap.\n",
    "    The remaining tests evaluates whether all the objects are visible.\n",
    "    Only the tests specifically required by the action pre/post-conditions should be run.\n",
    "    \n",
    "    Args:\n",
    "        pddl: Pddl instance.\n",
    "        s_partial: [A, 2, 2, N] (action, pre/post, pos/neg, state) Partial states for all actions. If provided, this function will test which actions are not covered by the tests.\n",
    "    Returns:\n",
    "        List of (idx_prop, lambda boxes: bool) tuples where bounding boxes should be passed into the lambda to evaluate the test condition.\n",
    "    \"\"\"\n",
    "    idx_props_visible = [pddl.state_index.get_proposition_index(f\"visible({obj})\") for obj in [\"a\", \"b\", \"c\", \"hand\"]]\n",
    "    idx_props_touching = [pddl.state_index.get_proposition_index(f\"touching({sth_a}, {sth_b})\") for sth_a, sth_b in [(\"a\", \"hand\"), (\"b\", \"hand\"), (\"c\", \"hand\"), (\"a\", \"b\"), (\"a\", \"c\"), (\"b\", \"c\")]]\n",
    "\n",
    "    tests_visible = [\n",
    "        (idx_props_visible[0], lambda boxes, hand, expected: is_sth_visible(boxes, 1, expected)),\n",
    "        (idx_props_visible[1], lambda boxes, hand, expected: is_sth_visible(boxes, 2, expected)),\n",
    "        (idx_props_visible[2], lambda boxes, hand, expected: is_sth_visible(boxes, 3, expected)),\n",
    "        (idx_props_visible[3], lambda boxes, hand, expected: is_sth_visible(boxes, 0, expected)),\n",
    "    ]\n",
    "    tests_touching = [\n",
    "        (idx_props_touching[0], lambda boxes, hand, expected: is_sth_touching_hand(boxes, 1, hand, expected)),\n",
    "        (idx_props_touching[1], lambda boxes, hand, expected: is_sth_touching_hand(boxes, 2, None, expected)),\n",
    "        (idx_props_touching[2], lambda boxes, hand, expected: is_sth_touching_hand(boxes, 3, None, expected)),\n",
    "        (idx_props_touching[3], lambda boxes, hand, expected: is_sth_touching_sth(boxes, 1, 2, expected)),\n",
    "        (idx_props_touching[4], lambda boxes, hand, expected: is_sth_touching_sth(boxes, 1, 3, expected)),\n",
    "        (idx_props_touching[5], lambda boxes, hand, expected: is_sth_touching_sth(boxes, 2, 3, expected)),\n",
    "    ]\n",
    "    \n",
    "    if s_partial is not None:\n",
    "        print(\"Actions not covered by tests:\")\n",
    "        idx_props = idx_props_touching + idx_props_visible\n",
    "        for id_action, action in enumerate(pddl.actions):\n",
    "            # [2, N]\n",
    "            s_pre = s_partial[id_action,0,...]\n",
    "            s_post = s_partial[id_action,1,...]\n",
    "            if not s_pre[:,idx_props].any():\n",
    "                print(id_action, actions[id_action])\n",
    "    \n",
    "    return tests_visible + tests_touching\n",
    "\n",
    "def evaluate_tests(\n",
    "    tests: typing.List[typing.Tuple[int, typing.Callable[[np.ndarray, hand_detector.Hand, bool], bool]]],\n",
    "    s_partial: np.ndarray,\n",
    "    boxes: np.ndarray,\n",
    "    hand: hand_detector.Hand,\n",
    ") -> bool:\n",
    "    \"\"\"Evaluates whether the propositions given by the tests are satisfied in the partial states.\n",
    "    \n",
    "    Any proposition not specified in the partial state is assumed to pass its corresponding test.\n",
    "    \n",
    "    Args:\n",
    "        test: List of (idx_prop, lambda(oxes, hand, expectedd) -> bool) pairs.\n",
    "        s_partial: [2, N] (pos/neg, num_props) Partial state.\n",
    "        boxes: [4, 4] (hand/a/b/c, x1/y1/x2/y2) Bounding boxes of objects for the given frame.\n",
    "        hand: Detected hand.\n",
    "    Returns:\n",
    "        True if all the tests are satisfied, raises a PropositionTestFailure otherwise.\n",
    "    \"\"\"\n",
    "    s_pos = s_partial[0]\n",
    "    s_neg = s_partial[1]\n",
    "    \n",
    "    for idx_prop, test in tests:\n",
    "        if not s_pos[idx_prop] and not s_neg[idx_prop]:\n",
    "            # Proposition not specified in partial state, so don't test.\n",
    "            continue\n",
    "            \n",
    "        # Either pos or neg is true.\n",
    "        expected = s_pos[idx_prop]\n",
    "        test(boxes, hand, expected)\n",
    "    return True\n",
    "\n",
    "def precompute_tests(\n",
    "    tests: typing.List[typing.Tuple[int, typing.Callable[[np.ndarray, hand_detector.Hand, bool], bool]]],\n",
    "    boxes: np.ndarray,\n",
    "    hand: hand_detector.Hand,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Evaluates whether the propositions given by the tests are true.\n",
    "    \n",
    "    Any proposition not specified in the partial state is assumed to pass its corresponding test.\n",
    "    \n",
    "    Args:\n",
    "        test: List of (idx_prop, lambda(boxes, hand, expectedd) -> bool) pairs.\n",
    "        s_partial: [2, N] (pos/neg, num_props) Partial state.\n",
    "        boxes: [4, 4] (hand/a/b/c, x1/y1/x2/y2) Bounding boxes of objects for the given frame.\n",
    "        hand: Detected hand.\n",
    "    Returns:\n",
    "        [2, Q] (pos/neg, num_tests) Partial state over whether each test returns true.\n",
    "    \"\"\"\n",
    "    results = np.zeros((2, len(tests)), dtype=bool)\n",
    "    \n",
    "    # Iterate over all tests.\n",
    "    for idx_test, (idx_prop, test) in enumerate(tests):\n",
    "        try:\n",
    "            # Run test.\n",
    "            val = test(boxes, hand, None)\n",
    "        except PropositionUnknown as e:\n",
    "            # Leave partial state as 0.\n",
    "            continue\n",
    "\n",
    "        idx_pos_neg = 1 - val\n",
    "        results[idx_pos_neg, idx_test] = True\n",
    "    \n",
    "    return results\n",
    "\n",
    "def draw_hands(img: np.ndarray, detected_hands: typing.List[hand_detector.Hand]):\n",
    "    import PIL\n",
    "    \n",
    "    img = PIL.Image.fromarray(img)\n",
    "    draw = PIL.ImageDraw.Draw(img)\n",
    "\n",
    "    for hand in detected_hands:\n",
    "        draw.polygon(hand.palm().flatten().tolist(), outline=(0,255,0))\n",
    "        for xy in hand.palm():\n",
    "            box = np.concatenate([xy - 10, xy + 10], axis=0)\n",
    "            draw.ellipse(box.tolist(), outline=(255,0,0))\n",
    "        for finger in hand.fingers():\n",
    "            draw.line(finger.flatten().tolist(), fill=(255,0,255))\n",
    "            xy = finger[-1]\n",
    "            box = np.concatenate([xy - 15, xy + 15], axis=0)\n",
    "            draw.ellipse(box.tolist(), outline=(255,255,255))\n",
    "            \n",
    "    img = np.array(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def evaluate_hands(\n",
    "    paths: config.EnvironmentPaths,\n",
    "    video_label: twentybn.dataset.VideoLabel,\n",
    "    id_video: int,\n",
    ") -> typing.List[typing.List[np.ndarray]]:\n",
    "    \"\"\"Detects hands in the given video.\n",
    "    \n",
    "    Args:\n",
    "        paths: Environment paths.\n",
    "        video_label: 20BN label.\n",
    "        id_video: Video id.\n",
    "    Returns:\n",
    "        [T] (num_keyframes) list of [H] (num_hands) lists of [L, 2] (num_landmarks, x/y) float32 arrays.\n",
    "    \"\"\"\n",
    "    # Load video.\n",
    "    video_frames = video_utils.read_video(paths.data / \"videos\" / f\"{id_video}.webm\", video_label.keyframes)\n",
    "    \n",
    "    NUM_HANDS = 2\n",
    "    T = len(video_frames)\n",
    "    \n",
    "    # [T] (num_keyframes)\n",
    "    detected_hands = []\n",
    "    \n",
    "    # Create hand detector.\n",
    "    with hand_detector.HandDetector(static_image_mode=False, max_num_hands=NUM_HANDS, min_detection_confidence=0.5) as hands:\n",
    "        \n",
    "        # Iterate over all keyframes.\n",
    "        for t, img in enumerate(video_frames):\n",
    "            # [H] (num_hands) list of [L, 2] (num_landmarks, x/y) landmarks.\n",
    "            detected = [hand.landmarks for hand in hands.detect(img)]\n",
    "            detected_hands.append(detected)\n",
    "    \n",
    "    return detected_hands\n",
    "\n",
    "def load_detected_hands(landmarks: typing.List[np.ndarray]) -> typing.List[hand_detector.Hand]:\n",
    "    \"\"\"Loads detected hands output by `precompute_hands()`.\n",
    "    \n",
    "    Args:\n",
    "        landmarks: [H] (num_hands) list of [L, 2] (num_landmarks, x/y) landmark arrays.\n",
    "    Returns:\n",
    "        List of Hand objects.\n",
    "    \"\"\"\n",
    "    return [hand_detector.Hand(hand_landmark) for hand_landmark in landmarks]\n",
    "#     detected_hands = []\n",
    "#     for hand_landmarks in landmarks:\n",
    "#         if hand_landmarks[0, 0] < 0:\n",
    "#             break\n",
    "#         detected_hands.append(hand_detector.Hand(hand_landmarks))\n",
    "#     return detected_hands\n",
    "\n",
    "def evaluate_video_conditions(\n",
    "    paths: config.EnvironmentPaths,\n",
    "    pddl: symbolic.Pddl,\n",
    "    video_label: twentybn.dataset.VideoLabel,\n",
    "    hands: np.ndarray,\n",
    "    id_video: int,\n",
    "    s_partial: np.ndarray,\n",
    "    tests: typing.List[typing.Tuple[int, typing.Callable[[np.ndarray, hand_detector.Hand, bool], bool]]],\n",
    "    generate_video: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Evaluates the pre/post-conditions for the given video.\n",
    "    \n",
    "    Args:\n",
    "        paths: Environment paths.\n",
    "        pddl: Pddl instance.\n",
    "        video_label: 20BN label.\n",
    "        hands: [T] (num_keyframes) list of [H] (num_hands) lists of [L, 2] (num_landmarks, x/y) landmark arrays for given video.\n",
    "        id_video: Video id.\n",
    "        s_partial: [2, 2, N] (pre/post, pos/neg, num_props) Partial state for current action.\n",
    "        tests: Output of `generate_tests()`.\n",
    "        generate_video: Whether to generate a video with the object/hand detections.\n",
    "    Returns:\n",
    "        [2, T] (pre/post, num_frames) int array indicating whether the frame passes the condition tests (0: False, 1: True, -1: Unknown).\n",
    "    \"\"\"\n",
    "\n",
    "    # Get test propositions.\n",
    "    # [2, 2, N] -> [2, 2, Q] (pre/post, pos/neg, num_tests)\n",
    "    idx_props = [idx_prop for idx_prop, test in tests]\n",
    "    s_expected = s_partial[:, :, idx_props]\n",
    "    prop_labels = [\"pre\", \"post\"] + [pddl.state_index.get_proposition(idx_prop) for idx_prop in idx_props]\n",
    "    \n",
    "    # [2, 2, Q] -> [2, Q] (pre/post, num_tests)\n",
    "    s_expected_pos = s_expected[:, 0, :]\n",
    "    s_expected_neg = s_expected[:, 1, :]\n",
    "    \n",
    "    # Prepare output.\n",
    "    # [2, T] (pre/post, num_frames)\n",
    "    T = len(hands)\n",
    "    test_results = np.zeros((2, T), dtype=np.int8)\n",
    "    \n",
    "    if generate_video:\n",
    "        # Load video.\n",
    "        video_frames = video_utils.read_video(paths.data / \"videos\" / f\"{id_video}.webm\", video_label.keyframes)\n",
    "        \n",
    "        video_out = []\n",
    "    \n",
    "    # Iterate over all keyframes.\n",
    "    for t in range(T):\n",
    "        box_hand = video_label.boxes[t, 0, :]\n",
    "        if box_hand[0] >= 0:\n",
    "#                 xy1_hand = np.maximum(0, box_hand[:2].astype(np.int) - 100)\n",
    "#                 xy2_hand = np.minimum(img.shape[:2][::-1], (box_hand[2:] + 101).astype(np.int))\n",
    "#                 img_hand = img[xy1_hand[1]:xy2_hand[1], xy1_hand[0]:xy2_hand[0]]\n",
    "#                 detected_hands = hands.detect(img_hand, xy_offset=xy1_hand)\n",
    "            detected_hands = load_detected_hands(hands[t])\n",
    "        else:\n",
    "            detected_hands = []\n",
    "\n",
    "        # hand = detected_hands[0] if detected_hands else None\n",
    "        hand = identify_contained_hand(box_hand, detected_hands)\n",
    "\n",
    "        # Run tests.\n",
    "        # [2, Q] (pos/neg, num_tests)\n",
    "        s_results = precompute_tests(tests, video_label.boxes[t], hand)\n",
    "\n",
    "        # [2, Q] -> [Q]\n",
    "        s_results_pos = s_results[0, :]\n",
    "        s_results_neg = s_results[1, :]\n",
    "\n",
    "        # [2, Q] (pre/post, num_tests)\n",
    "        violated = (s_expected_pos & s_results_neg[None, :]) | (s_expected_neg & s_results_pos[None, :])\n",
    "        unknown = (s_expected_pos & ~s_results_pos[None, :]) | (s_expected_neg & ~s_results_neg[None, :])\n",
    "\n",
    "        # [2] (pre/post)\n",
    "        satisfied = np.ones((violated.shape[0],), dtype=np.int8)\n",
    "        satisfied[unknown.any(axis=1)] = -1\n",
    "        satisfied[violated.any(axis=1)] = 0\n",
    "        test_results[:, t] = satisfied\n",
    "\n",
    "        if generate_video:\n",
    "            # Load video frame.\n",
    "            img = video_frames[t]\n",
    "            \n",
    "            # Draw hands/bounding boxes.\n",
    "            img = draw_hands(img, detected_hands)\n",
    "            img = video_utils.draw_bounding_boxes(img, video_label.boxes[t], [\"hand\"] + video_label.objects)\n",
    "\n",
    "            # Convert condition test results to probabilities.\n",
    "            # [2]\n",
    "            p_conditions = test_results[:, t].astype(np.float32)\n",
    "            p_conditions[p_conditions < 0] = 0.5\n",
    "\n",
    "            # Convert test values to probabilities.\n",
    "            # [Q]\n",
    "            p_results = s_results_pos.astype(np.float32) - s_results_neg.astype(np.float32)\n",
    "            p_results = 0.5 * (p_results + 1)\n",
    "\n",
    "            # Show pre/post-condition timesteps if available.\n",
    "            prop_labels_t = prop_labels.copy()\n",
    "            if t in video_label.pre:\n",
    "                prop_labels_t[0] = \"pre   !!!!!!!!!!!!!!!!!!!!\"\n",
    "            elif t in video_label.post:\n",
    "                prop_labels_t[1] = \"post !!!!!!!!!!!!!!!!!!!!\"\n",
    "\n",
    "            # [2], [Q] -> [2 + Q]\n",
    "            p_predict = np.concatenate((p_conditions, p_results), axis=0)\n",
    "            img = video_utils.overlay_predictions(img, p_predict, prop_labels_t)\n",
    "            video_out.append(img)\n",
    "    \n",
    "    if generate_video:\n",
    "        video_utils.write_video(paths.data / \"labeled_videos\" / f\"{id_video}.webm\", video_out)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def initialize_hands():\n",
    "    \"\"\"Initializes global variables required for `precompute_hands()`.\n",
    "    \n",
    "    Each worker process only loads the dataset once and then re-uses them for each action.\n",
    "    \"\"\"\n",
    "    global paths, labels, pddl\n",
    "\n",
    "    paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "    labels = twentybn.dataset.Labels(paths.data / \"labels.hdf5\")\n",
    "\n",
    "    pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.env / \"problem.pddl\"))\n",
    "\n",
    "def initialize_tests():\n",
    "    \"\"\"Initializes global variables required for `process_action()`.\n",
    "    \n",
    "    Each worker process only loads the dataset once and then re-uses them for each action.\n",
    "    \"\"\"\n",
    "    global paths, labels, pddl, tests, hands\n",
    "\n",
    "    initialize_hands()\n",
    "\n",
    "    tests = generate_tests(pddl)\n",
    "    \n",
    "    with open(paths.data / \"hands.pkl\", \"rb\") as f:\n",
    "        hands = pickle.load(f)\n",
    "\n",
    "def precompute_hands(id_action: int) -> typing.Dict[int, typing.List[typing.List[np.ndarray]]]:\n",
    "    \"\"\"Precompues hand detections for all the videos for one action.\n",
    "    \n",
    "    Assumes `initialize_hands()` has already been called.\n",
    "    \n",
    "    Args:\n",
    "        id_action: Action id.\n",
    "    Returns:\n",
    "        Map from id_video to [T] (num_keyframes) list of [H] (num_hands) list of [L, 2] (num_landmarks, x/y) float32 array of hand detections.\n",
    "    \"\"\"\n",
    "    global paths, labels, pddl\n",
    "\n",
    "    action = str(pddl.actions[id_action])\n",
    "    s_partial = dnf_utils.get_partial_state(pddl, action)\n",
    "    \n",
    "    # Iterate over all videos of the action.\n",
    "    detected_hands = {}\n",
    "    id_videos = labels.actions[id_action].videos\n",
    "    for id_video in id_videos:\n",
    "        try:\n",
    "            detected_hands[id_video] = evaluate_hands(paths, labels.videos[id_video], id_video)\n",
    "        except Exception as e:\n",
    "            print(f\"id_action={id_action}:\\n{e}\")\n",
    "            with open(f\"{id_action}.log\", \"a\") as f:\n",
    "                f.write(f\"{id_video}:\\n{e}\\n\")\n",
    "    \n",
    "    return detected_hands\n",
    "\n",
    "def process_action(id_action: int, generate_video: bool = False, num_videos: typing.Optional[int] = None) -> typing.Dict[int, np.ndarray]:\n",
    "    \"\"\"Checks the pre/post-conditions for all the videos for one action.\n",
    "    \n",
    "    Assumes `initialize_tests()` has already been called.\n",
    "    \n",
    "    Args:\n",
    "        id_action: Action id.\n",
    "        generate_video: Whether to generate a video for visualization.\n",
    "        num_videos: Maximum number of videos per action to process.\n",
    "    Returns:\n",
    "        Map from id_video to [2, T] (pre/post, num_frames) int array indicating whether the frame passes the condition tests (0: False, 1: True, -1: Unknown).\n",
    "    \"\"\"\n",
    "    global paths, labels, pddl, tests, hands\n",
    "\n",
    "    action = str(pddl.actions[id_action])\n",
    "    s_partial = dnf_utils.get_partial_state(pddl, action)\n",
    "    \n",
    "    # Iterate over all videos of the action.\n",
    "    test_results = {}\n",
    "    id_videos = labels.actions[id_action].videos if num_videos is None else labels.actions[id_action].videos[:num_videos]\n",
    "    for id_video in id_videos:\n",
    "        try:\n",
    "            test_results[id_video] = evaluate_video_conditions(paths, pddl, labels.videos[id_video], hands[id_video], id_video, s_partial, tests, generate_video)\n",
    "        except Exception as e:\n",
    "            print(f\"id_action={id_action}:\\n{e}\")\n",
    "            with open(f\"{id_action}.log\", \"a\") as f:\n",
    "                f.write(f\"{id_video}:\\n{e}\\n\")\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tqdm\n",
    "\n",
    "def find_continuous_ones(x: np.ndarray, left_to_right: bool):\n",
    "    \"\"\"Finds the maximum run of consecutive ones in the array.\n",
    "        \n",
    "        Args:\n",
    "            x: 1d array.\n",
    "            left_to_right: Whether to break ties with elements from left to right\n",
    "        Returns:\n",
    "            Range of largest run of consecutive ones (idx_start, idx_end).\n",
    "    \"\"\"\n",
    "    x = np.concatenate((np.array([0]), x, np.array([0])))\n",
    "    diff = x[1:] - x[:-1]\n",
    "    start = np.squeeze(np.argwhere(diff == 1), axis=1)\n",
    "    end = np.squeeze(np.argwhere(diff == -1), axis=1)\n",
    "    \n",
    "    idx = np.arange(len(start))\n",
    "    if left_to_right:\n",
    "        idx = idx[::-1]\n",
    "    \n",
    "    if left_to_right:\n",
    "        unsorted = np.array([(len(start) - i, end[i] - start[i]) for i in range(len(start))], dtype=[(\"idx\", np.uint32), (\"val\", np.float32)])\n",
    "    else:\n",
    "        unsorted = np.array([(i, end[i] - start[i]) for i in range(len(start))], dtype=[(\"idx\", np.uint32), (\"val\", np.float32)])\n",
    "    idx_ranges = np.argsort(unsorted, order=(\"val\", \"idx\"))[::-1]\n",
    "    \n",
    "    ranges = np.stack((start[idx_ranges], end[idx_ranges]), axis=0)\n",
    "    return ranges\n",
    "\n",
    "def find_pre_post_boundaries(x_class: np.ndarray) -> typing.Tuple[typing.Optional[int], typing.Optional[int]]:\n",
    "    \"\"\"Finds the last certain pre-condition index and first certain post-condition index in their respective clusters.\n",
    "    \n",
    "    Args:\n",
    "        x: [T] float32 class predictions (0-1).\n",
    "    Returns:\n",
    "        (last pre index, first post index).\n",
    "    \"\"\"\n",
    "    pre_clusters = find_continuous_ones(x_class < 0.5, left_to_right=True)\n",
    "    post_clusters = find_continuous_ones(x_class > 0.5, left_to_right=False)\n",
    "#     print(\"pre_clusters:\", pre_clusters)\n",
    "#     print(\"post_clusters:\", post_clusters)\n",
    "    \n",
    "    # Relax the constraints if one of the clusters is empty.\n",
    "    if pre_clusters.size == 0:\n",
    "        pre_clusters = find_continuous_ones(x_class <= 0.5, left_to_right=True)\n",
    "    if post_clusters.size == 0:\n",
    "        post_clusters = find_continuous_ones(x_class >= 0.5, left_to_right=False)\n",
    "    \n",
    "    idx_pre = 0\n",
    "    idx_post = 0\n",
    "    if pre_clusters.size == 0 and post_clusters.size == 0:\n",
    "        # No clusters.\n",
    "        return (0, x_class.shape[0])\n",
    "    elif pre_clusters.size == 0:\n",
    "        # Only post cluster.\n",
    "        return (0, post_clusters[0, idx_post])\n",
    "    elif post_clusters.size == 0:\n",
    "        # Only pre cluster.\n",
    "        return (pre_clusters[1, idx_pre], x_class.shape[0])\n",
    "    \n",
    "    while pre_clusters[0, idx_pre] >= post_clusters[1, idx_post]:\n",
    "        if idx_pre >= pre_clusters.shape[1] - 1 and idx_post >= post_clusters.shape[1] - 1:\n",
    "            return None, None\n",
    "        \n",
    "        # Avoid going past the last cluster.\n",
    "        if idx_pre >= pre_clusters.shape[1] - 1:\n",
    "            idx_post += 1\n",
    "            continue\n",
    "        elif idx_post >= post_clusters.shape[1] - 1:\n",
    "            idx_pre += 1\n",
    "            continue\n",
    "        \n",
    "        # Keep the larger cluster.\n",
    "        size_pre = pre_clusters[1, idx_pre] - pre_clusters[0, idx_pre]\n",
    "        size_post = post_clusters[1, idx_post] - post_clusters[0, idx_post]\n",
    "        if size_pre > size_post:\n",
    "            idx_post += 1\n",
    "            continue\n",
    "        elif size_pre < size_post:\n",
    "            idx_pre += 1\n",
    "            continue\n",
    "\n",
    "        # Clusters have the same size. Advance the one with the larger succeeding cluster.\n",
    "        size_pre = pre_clusters[1, idx_pre + 1] - pre_clusters[0, idx_pre + 1]\n",
    "        size_post = post_clusters[1, idx_post + 1] - post_clusters[0, idx_post + 1]\n",
    "        if size_pre >= size_post:\n",
    "            idx_pre += 1\n",
    "            continue\n",
    "        elif size_post > size_pre:\n",
    "            idx_post += 1\n",
    "            continue\n",
    "    \n",
    "    # Make sure pre comes before post.\n",
    "    post_clusters[0, idx_post] = max(post_clusters[0, idx_post], pre_clusters[0, idx_pre])\n",
    "    pre_clusters[1, idx_pre] = min(pre_clusters[1, idx_pre], post_clusters[1, idx_post])\n",
    "    \n",
    "    return (pre_clusters[1, idx_pre], post_clusters[0, idx_post])\n",
    "\n",
    "def test_results_to_probabilities(test_results: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts [2, T] (pre/post) test results where {0=false, 1=true, -1=unknown}\n",
    "    to a [2, T] probability vector where {0=pre, 1=post, and 0.5=unknown}.\n",
    "    \n",
    "    Args:\n",
    "        test_results: [2, T] (pre/post, num_timesteps) int32 condition test results.\n",
    "    Returns:\n",
    "        [2, T] (pre/post, num_timesteps) float32 probability.\n",
    "    \"\"\"\n",
    "    x = np.array(test_results, dtype=np.float32)\n",
    "    x_pre = x[0]\n",
    "    x_post = x[1]\n",
    "    \n",
    "    # Set uncertain timesteps leaning to one side.\n",
    "    idx_maybe_pre_post = (x_pre < 0) & (x_post > 0)\n",
    "    idx_pre_maybe_post = (x_pre > 0) & (x_post < 0)\n",
    "    idx_not_pre_maybe_post = (x_pre == 0) & (x_post < 0)\n",
    "    idx_maybe_pre_not_post = (x_pre < 0) & (x_post == 0)\n",
    "    idx_not_pre_post = idx_maybe_pre_post | idx_not_pre_maybe_post\n",
    "    idx_pre_not_post = idx_pre_maybe_post | idx_maybe_pre_not_post\n",
    "    x[:, idx_not_pre_post] = np.array([0.25, 0.75])[:, None]\n",
    "    x[:, idx_pre_not_post] = np.array([0.75, 0.25])[:, None]\n",
    "    \n",
    "    # Set timesteps where both pre- and post-conditions are true.\n",
    "    x[:, (x == 1).all(axis=0)] = 0.5\n",
    "    \n",
    "    # Set timesteps where neither pre- nor post-conditions are known.\n",
    "    x[x < 0] = 0.5\n",
    "    \n",
    "    return x\n",
    "\n",
    "def find_pre_post_frames(test_results: np.ndarray) -> typing.Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes pre- and post-frames from the test results.\n",
    "    \n",
    "    Args:\n",
    "        test_results: [2, T] int32 condition test results.\n",
    "    Returns:\n",
    "        (pre-frames, post-frames).\n",
    "    \"\"\"\n",
    "    # [2, T]\n",
    "    x_prob = test_results_to_probabilities(test_results)\n",
    "#     print(\"x_prob:\", x_prob)\n",
    "    \n",
    "    # Only choose non-zero elements.\n",
    "    # [T]\n",
    "    idx_valid = (x_prob != 0).any(axis=0)\n",
    "    if (idx_valid == 0).all():\n",
    "        return np.zeros((0,), dtype=np.uint32), np.zeros((0,), dtype=np.uint32)\n",
    "    \n",
    "    # Find pre/post boundaries among non-zero elements.\n",
    "    # [NZ]\n",
    "    x_class = x_prob[1, idx_valid]\n",
    "    idx_nonzero = np.array(idx_valid.nonzero()[0], dtype=np.uint32)\n",
    "    idx_pre_post = find_pre_post_boundaries(x_class)\n",
    "    if idx_pre_post[0] is None or idx_pre_post[1] is None:\n",
    "        return np.zeros((0,), dtype=np.uint32), np.zeros((0,), dtype=np.uint32)\n",
    "#     print(\"idx_nonzero:\", idx_nonzero)\n",
    "#     print(\"idx_pre_post:\", idx_pre_post)\n",
    "    \n",
    "    # Set boundary as mean between last pre and first post index in NZ.\n",
    "    idx_boundary = int(0.5 * (idx_pre_post[0] + idx_pre_post[1]) + 0.5)\n",
    "#     print(\"idx_boundary:\", idx_boundary)\n",
    "    \n",
    "    # Convert NZ index to timestep.\n",
    "    assert idx_boundary <= len(idx_nonzero)\n",
    "    idx_boundary = idx_nonzero[min(len(idx_nonzero) - 1, idx_boundary)]\n",
    "    \n",
    "    # Set pre/post frames to uncertain frames (0.5) within the pre/post boundary.\n",
    "    # (num_pre, num_post)\n",
    "    idx_pre = idx_nonzero[(x_class <= 0.5) & (idx_nonzero < idx_boundary)]\n",
    "    idx_post = idx_nonzero[(x_class >= 0.5) & (idx_nonzero >= idx_boundary)]\n",
    "    \n",
    "    return idx_pre, idx_post\n",
    "\n",
    "def append_pre_post_to_dataset(results: typing.Dict[int, np.ndarray], paths: config.EnvironmentPaths, id_action: typing.Optional[int] = None):\n",
    "    \"\"\"Appends pre/post frames to the hdf5 dataset.\n",
    "    \n",
    "    Args:\n",
    "        results: Test results in `condition_test_results.pkl`.\n",
    "        paths: Environment paths.\n",
    "        id_action: Process only this action, if not None.\n",
    "    \"\"\"\n",
    "    with h5py.File(paths.data / \"labels.hdf5\", \"a\") as f:\n",
    "        if id_action is None:\n",
    "            id_videos = np.array(f[\"video_ids\"])\n",
    "        else:\n",
    "            id_videos = np.array(f[f\"actions/{id_action}/videos\"])\n",
    "\n",
    "        for id_video in tqdm.notebook.tqdm(id_videos):\n",
    "            idx_pre, idx_post = find_pre_post_frames(results[id_video])\n",
    "\n",
    "            grp_video = f[\"videos\"][str(id_video)]\n",
    "            if \"pre\" in grp_video:\n",
    "                del grp_video[\"pre\"]\n",
    "            if \"post\" in grp_video:\n",
    "                del grp_video[\"post\"]\n",
    "            grp_video.create_dataset(\"pre\", data=idx_pre, dtype=np.uint32)\n",
    "            grp_video.create_dataset(\"post\", data=idx_post, dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "NUM_WORKERS = 6\n",
    "\n",
    "# Process hand detections for all actions in parallel.\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_WORKERS, initializer=initialize_hands) as executor:\n",
    "    paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "    pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.env / \"problem.pddl\"))\n",
    "    A = len(pddl.actions)\n",
    "    futures = {}\n",
    "    \n",
    "    for id_action in range(A):\n",
    "        future = executor.submit(precompute_hands, id_action)\n",
    "        futures[future] = id_action\n",
    "        if id_action < NUM_WORKERS:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    hands = {}\n",
    "    with tqdm.notebook.tqdm(total=A) as loop:\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            id_action = futures[future]\n",
    "            try:\n",
    "                hands.update(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Exception for id_action={id_action}:\\n{e}\")\n",
    "            loop.update(1)\n",
    "\n",
    "# Save hand detections.\n",
    "with open(paths.data / \"hands.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hands, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate pre/post-condition tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "initialize_tests()\n",
    "test_results = {}\n",
    "for id_action in tqdm.notebook.tqdm(range(len(pddl.actions))):\n",
    "    test_results.update(process_action(id_action))\n",
    "with open(paths.data / \"condition_test_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "with open(paths.data / \"condition_test_results.pkl\", \"rb\") as f:\n",
    "    test_results = pickle.load(f)\n",
    "\n",
    "append_pre_post_to_dataset(test_results, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute condition test statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm.notebook\n",
    "\n",
    "import config\n",
    "\n",
    "def compute_condition_statistics(paths: config.EnvironmentPaths, train_set: typing.List[int], val_set: typing.List[int]):\n",
    "    \"\"\"Computes pre/post-condition statistics for the 20BN dataset.\n",
    "    \n",
    "    Args:\n",
    "        paths: Environment paths.\n",
    "        train_set: Video ids in the original 20BN train set.\n",
    "        val_set: Video ids in the original 20BN val set.\n",
    "    Returns:\n",
    "    | Video | Action | Dataset | Pre | Post |\n",
    "    \"\"\"\n",
    "    df = {\n",
    "        \"Video\": [],\n",
    "        \"Action\": [],\n",
    "        \"Dataset\": [],\n",
    "        \"Pre\": [],\n",
    "        \"Post\": [],\n",
    "    }\n",
    "    val_set = set(val_set)\n",
    "    with h5py.File(paths.data / \"labels.hdf5\", \"r\") as f:\n",
    "        grp_videos = f[\"videos\"]\n",
    "        video_ids = np.array(f[\"video_ids\"])\n",
    "        for id_video in tqdm.notebook.tqdm(video_ids):\n",
    "            grp_video = grp_videos[str(id_video)]\n",
    "            \n",
    "            id_action = int(grp_video.attrs[\"id_action\"])\n",
    "            \n",
    "            # Assume video is in either train or val set.\n",
    "            dataset = \"val\" if id_video in val_set else \"train\"\n",
    "            \n",
    "            pre = grp_video[\"pre\"].size\n",
    "            post = grp_video[\"post\"].size\n",
    "            \n",
    "            df[\"Video\"].append(id_video)\n",
    "            df[\"Action\"].append(id_action)\n",
    "            df[\"Dataset\"].append(dataset)\n",
    "            df[\"Pre\"].append(pre)\n",
    "            df[\"Post\"].append(post)\n",
    "    \n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "with open(paths.data/ \"train_set.pkl\", \"rb\") as f:\n",
    "    twentybn_train_set = pickle.load(f)\n",
    "with open(paths.data / \"val_set.pkl\", \"rb\") as f:\n",
    "    twentybn_val_set = pickle.load(f)\n",
    "\n",
    "stats = compute_condition_statistics(paths, twentybn_train_set, twentybn_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train, val, test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import symbolic\n",
    "from gpred import dnf_utils\n",
    "\n",
    "def generate_dataset_splits(\n",
    "    pddl: symbolic.Pddl,\n",
    "    stats: pd.DataFrame,\n",
    "    twentybn_train_set: typing.List[int],\n",
    "    twentybn_val_set: typing.List[int],\n",
    ") -> typing.Tuple[typing.List[int], typing.List[int], typing.List[int]]:\n",
    "    \"\"\"Generates train, val, and test sets.\n",
    "    \n",
    "    Train and val sets are taken from the original 20BN train set, randomly selected for each action.\n",
    "    Test set is taken from the original 20BN val set. The final splits are roughly (85, 15, 15).\n",
    "    \n",
    "    Args:\n",
    "        pddl: Pddl instance.\n",
    "        stats: Table output by `compute_condition_statistics()`.\n",
    "    Returns:\n",
    "        (train_set, val_set, test_set) 3-tuple.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    \n",
    "    TRAIN_VAL = 1 - 0.15 / 0.85  # Assume original train set is 0.85 of the total.\n",
    "    \n",
    "    train_set = []\n",
    "    val_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    A = len(pddl.actions)\n",
    "    df = {\n",
    "        \"Action\": [],\n",
    "        \"Distribution\": [],\n",
    "        \"Dataset\": [],\n",
    "    }\n",
    "\n",
    "    for id_action, action in enumerate(pddl.actions):\n",
    "        s_partial = dnf_utils.get_partial_state(pddl, str(action))\n",
    "        if s_partial.sum() == 0:\n",
    "            df[\"Action\"] += [id_action, id_action, id_action]\n",
    "            df[\"Distribution\"] += [0, 0, 0]\n",
    "            df[\"Dataset\"] += [\"train\", \"val\", \"test\"]\n",
    "            continue\n",
    "\n",
    "        stats_a = stats[stats.Action == id_action]\n",
    "        stats_a = stats_a[(stats_a.Pre > 0) & (stats_a.Post > 0)]\n",
    "\n",
    "        train_val_ids = list(stats_a.Video[stats_a.Dataset == \"train\"])\n",
    "        test_ids = list(stats_a.Video[stats_a.Dataset == \"val\"])\n",
    "        \n",
    "        random.shuffle(train_val_ids)\n",
    "        train_val_split = int(TRAIN_VAL * len(train_val_ids) + 0.5)\n",
    "        train_ids = train_val_ids[:train_val_split]\n",
    "        val_ids = train_val_ids[train_val_split:]\n",
    "\n",
    "        train_set += train_ids\n",
    "        val_set += val_ids\n",
    "        test_set += test_ids\n",
    "        \n",
    "        num_train, num_val, num_test = len(train_ids), len(val_ids), len(test_ids)\n",
    "        num_total = num_train + num_val + num_test\n",
    "        df[\"Action\"] += [id_action, id_action, id_action]\n",
    "        df[\"Distribution\"] += [num_train / num_total, num_val / num_total, num_test / num_total]\n",
    "        df[\"Dataset\"] += [\"train\", \"val\", \"test\"]\n",
    "    \n",
    "    plt.subplots(figsize=(5, 40))\n",
    "    sns.barplot(data=df, y=\"Action\", x=\"Distribution\", hue=\"Dataset\", orient=\"h\")\n",
    "    plt.xlabel(\"Proportion of dataset\")\n",
    "    plt.ylabel(\"Action\")\n",
    "    plt.title(\"Dataset distribution\")\n",
    "    \n",
    "    # Preserve original dataset order.\n",
    "    train_set, val_set, test_set = set(train_set), set(val_set), set(test_set)\n",
    "    train_set = [id_video for id_video in twentybn_train_set if id_video in train_set]\n",
    "    val_set = [id_video for id_video in twentybn_train_set if id_video in val_set]\n",
    "    test_set = [id_video for id_video in twentybn_val_set if id_video in test_set]\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pddl = symbolic.Pddl(str(paths.domain_pddl), str(paths.problem_pddl))\n",
    "\n",
    "train_set, val_set, test_set = generate_dataset_splits(pddl, stats, twentybn_train_set, twentybn_val_set)\n",
    "\n",
    "print(f\"Train: {len(train_set)}\")\n",
    "print(f\"Val: {len(val_set)}\")\n",
    "print(f\"Test: {len(test_set)}\")\n",
    "\n",
    "with open(paths.data / \"dataset_splits.pkl\", \"wb\") as f:\n",
    "    pickle.dump((train_set, val_set, test_set), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze condition statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.subplots(figsize=(5, 40))\n",
    "\n",
    "stats[[\"Action\"]] \\\n",
    "    .assign(Pre = stats.Pre > 0, Post = stats.Post > 0) \\\n",
    "    .groupby(\"Action\", as_index=False) \\\n",
    "    .mean() \\\n",
    "    .melt(id_vars=\"Action\", value_vars=[\"Pre\",\"Post\"], var_name=\"Condition\") \\\n",
    "    .pipe((sns.barplot, \"data\"), y=\"Action\", x=\"value\", hue=\"Condition\", orient=\"h\")\n",
    "\n",
    "plt.savefig(\"figures/pre_post.png\", bbox_inches=\"tight\", transparent=\"True\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Partial: {len(stats[(stats.Pre > 0) | (stats.Post > 0)])} / {len(stats)}\")\n",
    "print(f\"Complete: {len(stats[(stats.Pre > 0) & (stats.Post > 0)])} / {len(stats)}\")\n",
    "print(f\"Train: {len(stats[(stats.Dataset == 'train') & (stats.Pre > 0) & (stats.Post > 0)])} / {len(stats[stats.Dataset == 'train'])}\")\n",
    "print(f\"Val: {len(stats[(stats.Dataset == 'val') & (stats.Pre > 0) & (stats.Post > 0)])} / {len(stats[stats.Dataset == 'val'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize condition tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort actions by proportion of videos with pre/post frames (from lowest to highest).\n",
    "\n",
    "id_actions = np.array(stats[[\"Action\"]] \\\n",
    "    .assign(Pre = stats.Pre > 0, Post = stats.Post > 0) \\\n",
    "    .groupby(\"Action\", as_index=False) \\\n",
    "    .mean() \\\n",
    "    .melt(id_vars=\"Action\", value_vars=[\"Pre\",\"Post\"], var_name=\"Condition\") \\\n",
    "    [[\"Action\", \"value\"]] \\\n",
    "    .groupby(\"Action\", as_index=False) \\\n",
    "    .min() \\\n",
    "    .sort_values(\"value\") \\\n",
    "    [[\"Action\"]]).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate videos for 30 worst-performing actions.\n",
    "\n",
    "\" \".join(str(id_action) for id_action in id_actions[:30])\n",
    "\n",
    "for id_action in tqdm.notebook.tqdm(id_actions[:30]):\n",
    "    process_action(id_action, generate_video=True, num_videos=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_videos = paths.data / \"labeled_videos\"\n",
    "id_videos = [int(p.stem) for p in path_videos.iterdir() if p.suffix in {\".mp4\", \".webm\"}]\n",
    "\n",
    "mini_action_instances = [[] for _ in range(len(pddl.actions))]\n",
    "for id_video in id_videos:\n",
    "    id_action = labels.videos[id_video].id_action\n",
    "    mini_action_instances[id_action].append(id_video)\n",
    "\n",
    "display_video_grid(labels, mini_action_instances, paths.data / \"labeled_videos\", num_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_action = 60\n",
    "\n",
    "pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.env / \"problem.pddl\"))\n",
    "test_results.update(process_action(id_action))\n",
    "with open(paths.data / \"condition_test_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_results, f)\n",
    "\n",
    "append_pre_post_to_dataset(test_results, paths, id_action=id_action)\n",
    "_ = process_action(id_action, generate_video=True, num_videos=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_video = 43454\n",
    "find_pre_post_frames(test_results[id_video])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import config\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "\"\"\"\n",
    "action_labels = [\n",
    "    {\n",
    "        \"label\": \"Approaching something with your camera\",\n",
    "        \"template\": \"Approaching [something] with your camera\",\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "with open(paths.data / \"action_labels.pkl\", \"rb\") as f:\n",
    "    action_labels = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "action_instances = [\n",
    "    [{id_video}, ...]\n",
    "]\n",
    "\"\"\"\n",
    "with open(paths.data / \"action_instances.pkl\", \"rb\") as f:\n",
    "    action_instances = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "video_labels = {\n",
    "    {id_video}: {\n",
    "        \"id_action\": id_action,\n",
    "        \"placeholders\": [\"a potato\", \"a vicks vaporub bottle\"],\n",
    "        \"objects\": [\"potato\", \"bottle\"],\n",
    "        \"frames\": {\n",
    "            idx_frame: {\n",
    "                \"{id_object/hand}\": [[x1, y1], [x2, y2]],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "with open(paths.data / \"video_labels.pkl\", \"rb\") as f:\n",
    "    video_labels = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "train_set = [{id_video}, ...]\n",
    "\"\"\"\n",
    "with open(paths.data / \"train_set.pkl\", \"rb\") as f:\n",
    "    train_set = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "val_set = [{id_video}, ...]\n",
    "\"\"\"\n",
    "with open(paths.data / \"val_set.pkl\", \"rb\") as f:\n",
    "    val_set = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "video_ranges = {\n",
    "    {id_video}: (\n",
    "        [idx_pre_frames, ...],\n",
    "        [idx_post_frames, ...]\n",
    "    )\n",
    "}\n",
    "\"\"\"\n",
    "with open(paths.data / \"video_ranges.pkl\", \"rb\") as f:\n",
    "    video_ranges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate hdf5 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import pathlib\n",
    "import random\n",
    "import typing\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "try:\n",
    "    tqdm\n",
    "except:\n",
    "    import tqdm.notebook as tqdm\n",
    "\n",
    "from env import twentybn\n",
    "from gpred import video_utils\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\"\"\"\n",
    "dataset.hdf5 = {\n",
    "  \"id_video\": [\n",
    "    \"images\": [2, 3 + num_objects, H, W], uint8\n",
    "    \"boxes\": [num_objects, 4] (x1, y1, x2, y2), float32\n",
    "  ],\n",
    "  \"videos\": [int, ...],\n",
    "  \"actions\": [int, ...]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def collect_written_videos(f: h5py.File, dataset: typing.List[int]):\n",
    "    \"\"\"Record output videos and actions to the dataset.\n",
    "    \n",
    "    Args:\n",
    "        f: Dataset file.\n",
    "        dataset: List of video ids from the train/val set.\n",
    "    \"\"\"\n",
    "    videos = []\n",
    "    actions = []\n",
    "    for id_video in dataset:\n",
    "        if not str(id_video) in f.keys():\n",
    "            continue\n",
    "        videos.append(id_video)\n",
    "        actions.append(video_labels[id_video][\"id_action\"])\n",
    "\n",
    "    dset_videos = f.create_dataset(\"videos\", (len(videos),), dtype=np.uint32)\n",
    "    dset_videos[:] = videos\n",
    "    dset_actions = f.create_dataset(\"actions\", (len(videos),), dtype=np.uint8)\n",
    "    dset_actions[:] = actions\n",
    "\n",
    "def extract_pre_post(dataset: typing.List[int], filename: str, path: pathlib.Path) -> typing.Dict[int, typing.Tuple]:\n",
    "    \"\"\"Extract pre/post frames and save them to an hdf5 dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: List of video ids from the train/val set.\n",
    "        filename: Name of dataset. The file will be saved as filename.hdf5.\n",
    "        path: Path of dataset.\n",
    "    \"\"\"\n",
    "    def extract_pre_post_worker(id_video: int):\n",
    "        if id_video not in video_ranges:\n",
    "            return\n",
    "\n",
    "        # Get pre/post video frames.\n",
    "        path_video = path / f\"videos/{id_video}.webm\"\n",
    "        keyframes = list(video_labels[id_video][\"frames\"].keys())\n",
    "        pre_frames, post_frames = video_ranges[id_video]\n",
    "        selected_keyframes = [random.choice(pre_frames), random.choice(post_frames)]\n",
    "        \n",
    "        try:\n",
    "            pre_post_frames = video_utils.read_video(path_video, selected_keyframes)\n",
    "        except:\n",
    "            return\n",
    "        if pre_post_frames is None:\n",
    "            return\n",
    "\n",
    "        # Write pre/post frames to dataset.\n",
    "        height, width = pre_post_frames.shape[1:3]\n",
    "        masks, indexed_boxes = twentybn.utils.create_bbox_masks(id_video, (height, width), video_labels, selected_keyframes)\n",
    "        boxes = indexed_boxes[:, :, 1:]\n",
    "        \n",
    "        # ([2, 3, H, W], [2, 4, H, W]) => [2, 7, H, W]\n",
    "        images = np.concatenate((np.moveaxis(pre_post_frames, 3, 1), masks), axis=1)\n",
    "        \n",
    "        grp = f.create_group(str(id_video))\n",
    "        dset_images = grp.create_dataset(\"images\", images.shape, dtype=np.uint8)\n",
    "        dset_images[...] = images\n",
    "        dset_boxes = grp.create_dataset(\"boxes\", boxes.shape, dtype=np.float32)\n",
    "        dset_boxes[...] = boxes\n",
    "    \n",
    "    with h5py.File(path / f\"{filename}.hdf5\", \"w\") as f:\n",
    "        for id_video in tqdm.tqdm(dataset):\n",
    "            extract_pre_post_worker(id_video)\n",
    "#         with concurrent.futures.ThreadPoolExecutor(60) as pool:\n",
    "#             futures = [pool.submit(extract_pre_post_worker, id_video) for id_video in dataset]\n",
    "#             with tqdm.tqdm(total=len(dataset)) as pbar:\n",
    "#                 for result in concurrent.futures.as_completed(futures):\n",
    "#                     pbar.update(1)\n",
    "        \n",
    "        collect_written_videos(f, dataset)\n",
    "\n",
    "#extract_pre_post(train_set[:10000], \"pre_post_train_mini\", paths.data)\n",
    "#extract_pre_post(val_set[:10000], \"pre_post_val_mini\", paths.data)\n",
    "extract_pre_post(train_set, \"pre_post_train\", paths.data)\n",
    "extract_pre_post(val_set, \"pre_post_val\", paths.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicate.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import h5py\n",
    "import hdf5plugin\n",
    "import numpy as np\n",
    "import tqdm.notebook\n",
    "\n",
    "import symbolic\n",
    "from gpred import video_utils\n",
    "from env import twentybn\n",
    "\n",
    "def create_predicate_dataset(\n",
    "    pddl: symbolic.Pddl,\n",
    "    labels: twentybn.dataset.Labels,\n",
    "    dataset: typing.List[int],\n",
    "    filename: str,\n",
    "    path: pathlib.Path\n",
    "):\n",
    "    \"\"\"Extracts pre/post frames and save them to an hdf5 dataset.\n",
    "    \n",
    "    Args:\n",
    "        pddl: Pddl instance.\n",
    "        labels: 20BN labels.\n",
    "        dataset: List of video ids from the train/val set.\n",
    "        filename: Name of dataset. The file will be saved as filename.hdf5.\n",
    "        path: Path of dataset.\n",
    "    \"\"\"\n",
    "    def extract_pre_post_worker(id_video: int):\n",
    "        video_label = labels.videos[id_video]\n",
    "        if video_label.pre.size == 0 or video_label.post.size == 0:\n",
    "            return\n",
    "\n",
    "        # Get pre/post video frames.\n",
    "        path_video = path / f\"videos/{id_video}.webm\"\n",
    "        keyframes = video_label.keyframes[np.concatenate((video_label.pre, video_label.post))]\n",
    "        images = video_utils.read_video(path_video, keyframes)\n",
    "        \n",
    "        t_post = len(video_label.pre)\n",
    "        pre_images = images[:t_post]\n",
    "        post_images = images[t_post:]\n",
    "        \n",
    "        # [T, 16, 3, 4] (num_selected_frames, num_arg_combos, roi/arg_a/arg_b, x1/y1/x2/y2)\n",
    "        pre_boxes = twentybn.utils.split_bbox_args(pddl, video_label, video_label.pre)\n",
    "        post_boxes = twentybn.utils.split_bbox_args(pddl, video_label, video_label.post)\n",
    "        \n",
    "        grp = f.create_group(str(id_video))\n",
    "        dset_pre_frames = grp.create_dataset(\"pre_frames\", data=video_label.pre, dtype=np.uint32)\n",
    "        dset_post_frames = grp.create_dataset(\"post_frames\", data=video_label.post, dtype=np.uint32)\n",
    "        \n",
    "        H, W = pre_images[0].shape[:2]\n",
    "        T_pre = len(pre_images)\n",
    "        T_post = len(post_images)\n",
    "#         print(len(pre_images), pre_images[0].shape, (T_pre, H, W, 3))\n",
    "        dset_pre_images = grp.create_dataset(\"pre_images\", data=pre_images, shape=(T_pre, H, W, 3), chunks=(1, H, W, 3), dtype=np.uint8, **hdf5plugin.Blosc(cname=\"lz4hc\"))\n",
    "        dset_post_images = grp.create_dataset(\"post_images\", data=post_images, shape=(T_post, H, W, 3), chunks=(1, H, W, 3), dtype=np.uint8, **hdf5plugin.Blosc(cname=\"lz4hc\"))\n",
    "        \n",
    "        dset_pre_boxes = grp.create_dataset(\"pre_boxes\", data=pre_boxes, dtype=np.float32)\n",
    "        dset_post_boxes = grp.create_dataset(\"post_boxes\", data=post_boxes, dtype=np.float32)\n",
    "    \n",
    "    def collect_written_videos(f: h5py.File, dataset: typing.List[int]):\n",
    "        \"\"\"Record output videos and actions to the dataset.\n",
    "\n",
    "        Args:\n",
    "            f: Dataset file.\n",
    "            dataset: List of video ids from the train/val set.\n",
    "        \"\"\"\n",
    "        videos = []\n",
    "        actions = []\n",
    "        for id_video in dataset:\n",
    "            if not str(id_video) in f.keys():\n",
    "                continue\n",
    "            videos.append(id_video)\n",
    "            actions.append(labels.videos[id_video].id_action)\n",
    "\n",
    "        dset_videos = f.create_dataset(\"videos\", data=videos, dtype=np.uint32)\n",
    "        dset_actions = f.create_dataset(\"actions\", data=actions, dtype=np.uint32)\n",
    "    \n",
    "    with h5py.File(path / f\"{filename}.hdf5\", \"w\") as f:\n",
    "        for id_video in tqdm.notebook.tqdm(dataset):\n",
    "            extract_pre_post_worker(id_video)\n",
    "        \n",
    "        collect_written_videos(f, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import config\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "pddl = symbolic.Pddl(str(paths.domain_pddl), str(paths.problem_pddl))\n",
    "\n",
    "labels = twentybn.dataset.Labels()\n",
    "\n",
    "with open(paths.data / \"dataset_splits.pkl\", \"rb\") as f:\n",
    "    train_set, val_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_predicate_dataset(pddl, labels, train_set, \"predicate_train\", paths.data)\n",
    "create_predicate_dataset(pddl, labels, val_set, \"predicate_val\", paths.data)\n",
    "create_predicate_dataset(pddl, labels, test_set, \"predicate_test\", paths.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import symbolic\n",
    "import tqdm\n",
    "\n",
    "from env import twentybn\n",
    "from gpred import video_utils\n",
    "import config\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.env / \"problem.pddl\"))\n",
    "\n",
    "\"\"\"\n",
    "dataset.hdf5 = {\n",
    "  \"id_video\": [\n",
    "    \"pre_frames\": [num_pre_frames], int,\n",
    "    \"post_frames\": [num_post_frames], int,\n",
    "    \"pre_images\": [num_pre_frames, H, W, 3], uint8,\n",
    "    \"post_images\": [num_post_frames, H, W, 3], uint8,\n",
    "    \"pre_boxes\": [num_pre_frames, num_arg_combos, 3, 4] (roi/arg_a/arg_b, x1/y1/x2/y2), float32,\n",
    "    \"pre_boxes\": [num_post_frames, num_arg_combos, 3, 4] (roi/arg_a/arg_b, x1/y1/x2/y2), float32,\n",
    "  ],\n",
    "  \"videos\": [int, ...],\n",
    "  \"actions\": [int, ...]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#create_predicate_dataset(train_set[:10000], \"predicate_train_mini\", paths.data)\n",
    "#create_predicate_dataset(val_set[:1000], \"predicate_val_mini\", paths.data)\n",
    "#create_predicate_dataset(train_set, \"predicate_train\", paths.data)\n",
    "#create_predicate_dataset(val_set, \"predicate_val\", paths.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import symbolic\n",
    "import tqdm\n",
    "\n",
    "from env import twentybn\n",
    "from gpred import video_utils\n",
    "import config\n",
    "\n",
    "paths = config.EnvironmentPaths(environment=\"twentybn\")\n",
    "\n",
    "pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.env / \"problem.pddl\"))\n",
    "\n",
    "\"\"\"\n",
    "dataset.hdf5 = {\n",
    "  \"id_video\": [\n",
    "    \"pre_frames\": [num_pre_frames], int,\n",
    "    \"post_frames\": [num_post_frames], int,\n",
    "    \"pre_images\": [num_pre_frames, H, W, 3], uint8,\n",
    "    \"post_images\": [num_post_frames, H, W, 3], uint8,\n",
    "    \"pre_boxes\": [num_pre_frames, num_arg_combos, 3, 4] (roi/arg_a/arg_b, x1/y1/x2/y2), float32,\n",
    "    \"pre_boxes\": [num_post_frames, num_arg_combos, 3, 4] (roi/arg_a/arg_b, x1/y1/x2/y2), float32,\n",
    "  ],\n",
    "  \"videos\": [int, ...],\n",
    "  \"actions\": [int, ...]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def collect_written_videos(f: h5py.File, dataset: typing.List[int]):\n",
    "    \"\"\"Record output videos and actions to the dataset.\n",
    "    \n",
    "    Args:\n",
    "        f: Dataset file.\n",
    "        dataset: List of video ids from the train/val set.\n",
    "    \"\"\"\n",
    "    videos = []\n",
    "    actions = []\n",
    "    for id_video in dataset:\n",
    "        if not str(id_video) in f.keys():\n",
    "            continue\n",
    "        videos.append(id_video)\n",
    "        actions.append(video_labels[id_video][\"id_action\"])\n",
    "\n",
    "    dset_videos = f.create_dataset(\"videos\", (len(videos),), dtype=np.uint32)\n",
    "    dset_videos[:] = videos\n",
    "    dset_actions = f.create_dataset(\"actions\", (len(videos),), dtype=np.uint8)\n",
    "    dset_actions[:] = actions\n",
    "\n",
    "def create_predicate_dataset(labels: twentybn.dataset.Labels, dataset: typing.List[int], filename: str, path: pathlib.Path):\n",
    "    \"\"\"Extracts pre/post frames and save them to an hdf5 dataset.\n",
    "    \n",
    "    Args:\n",
    "        labels: 20BN labels.\n",
    "        dataset: List of video ids from the train/val set.\n",
    "        filename: Name of dataset. The file will be saved as filename.hdf5.\n",
    "        path: Path of dataset.\n",
    "    \"\"\"\n",
    "    def extract_pre_post_worker(id_video: int):\n",
    "        if id_video not in video_ranges:\n",
    "            return\n",
    "\n",
    "        # Get pre/post video frames.\n",
    "        path_video = path / f\"videos/{id_video}.webm\"\n",
    "        keyframes = list(video_labels[id_video][\"frames\"].keys())\n",
    "        pre_frames, post_frames = video_ranges[id_video]\n",
    "        \n",
    "        try:\n",
    "            # [T, H, W, 3]\n",
    "            pre_images = video_utils.read_video(path_video, pre_frames)\n",
    "            post_images = video_utils.read_video(path_video, post_frames)\n",
    "        except:\n",
    "            return\n",
    "        if pre_images is None or post_images is None:\n",
    "            return\n",
    "\n",
    "        # [T, 16, 3, 4] (num_selected_frames, num_arg_combos, roi/arg_a/arg_b, x1/y1/x2/y2)\n",
    "        pre_boxes = twentybn.utils.split_bbox_args(pddl, id_video, video_labels, pre_frames)\n",
    "        post_boxes = twentybn.utils.split_bbox_args(pddl, id_video, video_labels, post_frames)\n",
    "        \n",
    "        grp = f.create_group(str(id_video))\n",
    "        dset_pre_frames = grp.create_dataset(\"pre_frames\", (len(pre_frames),), dtype=int)\n",
    "        dset_pre_frames[...] = pre_frames\n",
    "        dset_post_frames = grp.create_dataset(\"post_frames\", (len(post_frames),), dtype=int)\n",
    "        dset_post_frames[...] = post_frames\n",
    "        \n",
    "        dset_pre_images = grp.create_dataset(\"pre_images\", pre_images.shape, dtype=np.uint8, compression=\"gzip\", compression_opts=4)\n",
    "        dset_pre_images[...] = pre_images\n",
    "        dset_post_images = grp.create_dataset(\"post_images\", post_images.shape, dtype=np.uint8, compression=\"gzip\", compression_opts=4)\n",
    "        dset_post_images[...] = post_images\n",
    "        \n",
    "        dset_pre_boxes = grp.create_dataset(\"pre_boxes\", pre_boxes.shape, dtype=np.float32)\n",
    "        dset_pre_boxes[...] = pre_boxes\n",
    "        dset_post_boxes = grp.create_dataset(\"post_boxes\", post_boxes.shape, dtype=np.float32)\n",
    "        dset_post_boxes[...] = post_boxes\n",
    "    \n",
    "    with h5py.File(path / f\"{filename}.hdf5\", \"w\") as f:\n",
    "        for id_video in tqdm.notebook.tqdm(dataset):\n",
    "            extract_pre_post_worker(id_video)\n",
    "        \n",
    "        collect_written_videos(f, dataset)\n",
    "\n",
    "#extract_pre_post(train_set[:10000], \"pre_post_train_mini\", paths.data)\n",
    "#extract_pre_post(val_set[:10000], \"pre_post_val_mini\", paths.data)\n",
    "\n",
    "#create_predicate_dataset(train_set[:10000], \"predicate_train_mini\", paths.data)\n",
    "#create_predicate_dataset(val_set[:1000], \"predicate_val_mini\", paths.data)\n",
    "#create_predicate_dataset(train_set, \"predicate_train\", paths.data)\n",
    "#create_predicate_dataset(val_set, \"predicate_val\", paths.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "from gpred import dnf_utils\n",
    "\n",
    "\n",
    "def plot_predicate_counts(stats: pd.DataFrame):\n",
    "    \"\"\"Plots predicates (x) vs. count (y).\n",
    "    \n",
    "    Args:\n",
    "        stats: Longform dataframe output by `compute_pddl_statistics()`.\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    g = sns.countplot(data=stats.sort_values(\"Predicate\"), x=\"Predicate\", hue=\"Label\")\n",
    "    for item in g.get_xticklabels():\n",
    "        item.set_rotation(90)\n",
    "\n",
    "def plot_dnfs(stats: pd.DataFrame):\n",
    "    \"\"\"Plots a heatmap of actions vs. propositions specified by their DNFs.\n",
    "    \n",
    "    Args:\n",
    "        stats: Longform table output by compute_pddl_statistics().\n",
    "    \"\"\"\n",
    "    SIZE_SECTION = 10\n",
    "    CMAP = sns.diverging_palette(10, 130, n=100)\n",
    "    \n",
    "    df_action_v_prop = stats.astype({\"Label\": float}).pivot(index=[\"Action\", \"Condition\"], columns=\"Proposition\", values=\"Label\")\n",
    "    num_rows = len(df_action_v_prop)\n",
    "    num_sections = math.ceil(num_rows / SIZE_SECTION)\n",
    "\n",
    "    f, axs = plt.subplots(num_sections, 1, figsize=(25, num_sections * 5))\n",
    "\n",
    "    for i in tqdm.notebook.tqdm(range(num_sections)):\n",
    "        plt.subplot(num_sections, 1, i + 1)\n",
    "        g = sns.heatmap(data=df_action_v_prop[i*SIZE_SECTION:min(len(df_action_v_prop), (i+1)*SIZE_SECTION)], square=True, cmap=CMAP, linewidths=0.5, linecolor=\"#eee\", cbar_kws={\"shrink\": 0.5})\n",
    "        \n",
    "def plot_predicate_weights(w: np.ndarray):\n",
    "    \"\"\"Plots predicates (x) vs. weight (y).\n",
    "    \n",
    "    Args:\n",
    "        stats: Longform dataframe output by `compute_pddl_statistics()`.\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    df = pd.DataFrame(w.T, columns=[\"Pos\", \"Neg\"], index=[str(pred) for pred in pddl.predicates])\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df = pd.melt(df, id_vars=[\"index\"], value_vars=[\"Pos\",\"Neg\"])\n",
    "    df = df.rename(columns={\"index\": \"Predicate\", \"variable\": \"Label\", \"value\": \"Weight\"})\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    g = sns.barplot(data=df.sort_values(\"Predicate\"), x=\"Predicate\", y=\"Weight\", hue=\"Label\")\n",
    "    for item in g.get_xticklabels():\n",
    "        item.set_rotation(90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(paths.data / \"predicate_val.hdf5\",\"r\") as f:\n",
    "    actions = [str(action) for action in pddl.actions]\n",
    "    action_instances = [actions[idx_action] for idx_action in f[\"actions\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = dnf_utils.compute_pddl_statistics(pddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddl = symbolic.Pddl(str(paths.env / \"domain.pddl\"), str(paths.problem_pddl))\n",
    "stats = dnf_utils.compute_pddl_statistics(pddl)\n",
    "\n",
    "plot_predicate_counts(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = dnf_utils.compute_predicate_class_weights(pddl, action_instances=action_instances)\n",
    "plot_predicate_weights(np.minimum(1, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_inv = dnf_utils.compute_predicate_class_weights(pddl)\n",
    "plot_predicate_weights(w_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dnfs(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find video resolution ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with h5py.File(paths.data / \"pre_post_train.hdf5\", \"r\") as f:\n",
    "    H_max = 0\n",
    "    H_min = 10000\n",
    "    W_max = 0\n",
    "    W_min = 10000\n",
    "    for id_video in tqdm.tqdm(f[\"videos\"]):\n",
    "        dim = f[str(id_video)][\"images\"].shape[2:]\n",
    "        H_min = min(H_min, dim[0])\n",
    "        H_max = max(H_max, dim[0])\n",
    "        W_min = min(W_min, dim[1])\n",
    "        W_max = max(W_max, dim[1])\n",
    "\n",
    "with h5py.File(paths.data / \"pre_post_val.hdf5\", \"r\") as f:\n",
    "    for id_video in tqdm.tqdm(f[\"videos\"]):\n",
    "        dim = f[str(id_video)][\"images\"].shape[2:]\n",
    "        H_min = min(H_min, dim[0])\n",
    "        H_max = max(H_max, dim[0])\n",
    "        W_min = min(W_min, dim[1])\n",
    "        W_max = max(W_max, dim[1])\n",
    "\n",
    "print(H_max, W_max, H_min, W_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List videos with mismatching placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for id_video, video_label in video_labels.items():\n",
    "    if len(video_label[\"objects\"]) != len(video_label[\"placeholders\"]):\n",
    "        if not video_label[\"id_action\"] in (102, 144):\n",
    "            print(id_video, video_label[\"id_action\"], video_label[\"objects\"], video_label[\"placeholders\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
